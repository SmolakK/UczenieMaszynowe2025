{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437e2b1e",
   "metadata": {},
   "source": [
    "# Uczenie Maszynowe - ćwiczenia 5\n",
    "\n",
    "Dziś poznamy nowe algorytmy uczenia maszynowego, dowiemy się czegoś o hiperparametrach modeli i zastosujemy je w praktyce!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712aad2e",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR)\n",
    "\n",
    "Support Vector Regression (SVR) to algorytm oparty na zasadach Support Vector Machines (SVM), stosowany do regresji. Celem SVR jest znalezienie linii, krzywej lub powierzchni, która najlepiej odwzorowuje zależność między zmiennymi wejściowymi a wyjściową. Oto kluczowe aspekty działania SVR:\n",
    "\n",
    "Wektory nośne (support vector) oznacza wykorzystanie punktów \"najistotniejszych\" do wpsowania modelu.\n",
    "\n",
    "### Zasada Epsilon-Tube\n",
    "\n",
    "SVR nie próbuje dokładnie dopasować modelu do każdego punktu danych. Zamiast tego, określa pewien margines błędu, zwany epsilon (ε), w którym model toleruje różnice między przewidywanymi a rzeczywistymi wartościami.\n",
    "\n",
    "Jeśli punkt danych znajduje się w tym marginesie, jest traktowany jako \"dobrze dopasowany\" i nie wpływa na ostateczne rozwiązanie.\n",
    "Punkty poza marginesem (leżące poza epsilon-tube) przyczyniają się do błędu i są karane w funkcji kosztu.\n",
    "\n",
    "### Optymalizacja i marginesy\n",
    "\n",
    "Celem SVR jest znalezienie takiej linii/krzywej/powierzchni, która:\n",
    "\n",
    "Minimalizuje odchylenie od danych, jednocześnie dopuszczając tolerancję ε.\n",
    "Maksymalizuje płaskość funkcji, co oznacza regularyzację.\n",
    "\n",
    "Formuła optymalizacji w SVR uwzględnia dwa aspekty:\n",
    "\n",
    "Minimalizację ∣∣w∣∣2 (ograniczanie modelu, aby był prosty).\n",
    "Minimalizację błędów dla punktów poza epsilon-tube (punkty trudne do przewidzenia są karane proporcjonalnie do ich odległości od marginesu).\n",
    "\n",
    "### Kara za odchylenia - parametr C\n",
    "\n",
    "Parametr C kontroluje kompromis między dopasowaniem modelu a jego ogólną złożonością:\n",
    "\n",
    "    Niskie C: Model jest bardziej elastyczny, dopuszcza większe błędy, co może skutkować niedopasowaniem (underfitting).\n",
    "    Wysokie C: Model bardziej stara się dopasować do danych treningowych, co może prowadzić do przeuczenia (overfitting).\n",
    "\n",
    "### Kernel i przestrzeń cech\n",
    "\n",
    "SVR może pracować z danymi nieliniowymi dzięki tzw. trikom jądrowym (kernel tricks):\n",
    "\n",
    "Kernel przekształca dane do wyższej przestrzeni wymiarów, gdzie można łatwiej znaleźć liniową hiperpowierzchnię (np. linię regresji).\n",
    "\n",
    "Popularne kernele to:\n",
    "        \n",
    "        Linear (prosta regresja liniowa),\n",
    "        Polynomial (model polinomiczny),\n",
    "        RBF (nieliniowy model z funkcją radialną, dobry dla złożonych danych).\n",
    "\n",
    "### Epsilon i funkcja kosztu\n",
    "\n",
    "Parametr ε (epsilon) definiuje szerokość marginesu tolerancji:\n",
    "\n",
    "    Mniejsze epsilon: Model stara się bardziej dopasować do danych, co może skutkować przeuczeniem.\n",
    "    Większe epsilon: Model jest bardziej tolerancyjny na błędy i ignoruje drobne odchylenia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f2fcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48799217b531422f8578803a81d20c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='C', min=0.1), FloatSlider(value=0.1, description='ep…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_svr(C=1.0, epsilon=0.1, kernel='rbf')>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(50, 1), axis=0)\n",
    "y = X.ravel() + 4 * (0.5 - np.random.rand(50))\n",
    "\n",
    "# Function to plot SVR with interactive controls\n",
    "def plot_svr(C=1.0, epsilon=0.1, kernel='rbf'):\n",
    "    # Train SVR model\n",
    "    svr = SVR(kernel=kernel, C=C, epsilon=epsilon)\n",
    "    svr.fit(X, y)\n",
    "    \n",
    "    # Generate predictions\n",
    "    X_fit = np.linspace(0, 5, 500).reshape(-1, 1)\n",
    "    y_fit = svr.predict(X_fit)\n",
    "    \n",
    "    # Plot data and SVR predictions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X, y, color='red', label='Data points')\n",
    "    plt.plot(X_fit, y_fit, color='blue', label='SVR prediction')\n",
    "    plt.fill_between(\n",
    "        X_fit.ravel(),\n",
    "        y_fit - epsilon,\n",
    "        y_fit + epsilon,\n",
    "        color='blue',\n",
    "        alpha=0.2,\n",
    "        label=f\"Epsilon-tube (ε={epsilon})\"\n",
    "    )\n",
    "    plt.title(f\"Support Vector Regression (Kernel: {kernel}, C={C}, ε={epsilon})\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widget\n",
    "interact(\n",
    "    plot_svr,\n",
    "    C=(0.1, 100.0, 0.1),\n",
    "    epsilon=(0.01, 1.0, 0.01),\n",
    "    kernel=['linear', 'poly', 'rbf']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23bdb9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385890fcd4b44533af2388418af9d66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='C', min=0.1), FloatSlider(value=0.1, description='ep…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_svr(C=1.0, epsilon=0.1, kernel='rbf')>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(50, 1), axis=0)\n",
    "y = np.sin(X).ravel() + 0.3 * (0.5 - np.random.rand(50))\n",
    "\n",
    "# Function to plot SVR with interactive controls\n",
    "def plot_svr(C=1.0, epsilon=0.1, kernel='rbf'):\n",
    "    # Train SVR model\n",
    "    svr = SVR(kernel=kernel, C=C, epsilon=epsilon)\n",
    "    svr.fit(X, y)\n",
    "    \n",
    "    # Generate predictions\n",
    "    X_fit = np.linspace(0, 5, 500).reshape(-1, 1)\n",
    "    y_fit = svr.predict(X_fit)\n",
    "    \n",
    "    # Plot data and SVR predictions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X, y, color='red', label='Data points')\n",
    "    plt.plot(X_fit, y_fit, color='blue', label='SVR prediction')\n",
    "    plt.fill_between(\n",
    "        X_fit.ravel(),\n",
    "        y_fit - epsilon,\n",
    "        y_fit + epsilon,\n",
    "        color='blue',\n",
    "        alpha=0.2,\n",
    "        label=f\"Epsilon-tube (ε={epsilon})\"\n",
    "    )\n",
    "    plt.title(f\"Support Vector Regression (Kernel: {kernel}, C={C}, ε={epsilon})\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widget\n",
    "interact(\n",
    "    plot_svr,\n",
    "    C=(0.1, 100.0, 0.1),\n",
    "    epsilon=(0.01, 1.0, 0.01),\n",
    "    kernel=['linear', 'poly', 'rbf']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b0ee1",
   "metadata": {},
   "source": [
    "# Zadanie 1.\n",
    "\n",
    "Poniżej znajdziesz wygenerowane dwa zestawy danych. Wykorzystaj algorytm SVR by jak najlepiej rozwiązać problem predykcji. Wyświetl wpasowany model w dane by lepiej zwizualizować zależność parametrów od zachowania algorytmu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a77ea37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "dataset1 = make_regression(n_samples=100,n_features=1,noise=3)\n",
    "dataset1_x = dataset1[0]\n",
    "dataset1_y = dataset1[1]\n",
    "dataset2 = make_regression(n_samples=100,n_features=1,noise=3)\n",
    "dataset2_x = dataset2[0]\n",
    "dataset2_y = dataset2[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9a0400c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2056eeae280>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(dataset1_x,dataset1_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8821065e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2056fee9220>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(dataset2_x,dataset2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a934a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df6f7874",
   "metadata": {},
   "source": [
    "# Zadanie 2.\n",
    "\n",
    "Wykorzystując dane z poprzednich ćwiczeń (i podział na zbiory oraz sprawdzian kryżowy), wykorzystaj algorytm SVR do predykcji. Wesprzyj się dokumentacją: https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVR.html\n",
    "\n",
    "2a. Sprawdź wynik SVR (dla domyślnych parametrów, ale różnych typów kernela) dla różnych kombinacji cech. Potwierdź czy model działa najlepiej dla wcześniej wybranego zestawu cech czy też może zauważalna jest różnica.\n",
    "2b. Dla wybranego zestawu cech, sprawdź różne kombinacje hiperparametrów modelu, w tym kernel, C, epsilon oraz gamma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40671a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89e010eb",
   "metadata": {},
   "source": [
    "# **Regresja k-NN (k-Nearest Neighbors Regression)**\n",
    "\n",
    "Regresja k-NN (ang. *k-Nearest Neighbors Regression*) to prosty, ale potężny algorytm uczenia maszynowego, który wykorzystuje odległości między punktami danych do przewidywania wartości ciągłych. Jest to metoda nieparametryczna, co oznacza, że nie zakłada konkretnej formy zależności między cechami a wartościami wyjściowymi.\n",
    "\n",
    "---\n",
    "\n",
    "## **Jak działa algorytm k-NN w regresji?**\n",
    "\n",
    "1. **Zbieranie danych treningowych**:\n",
    "   - Algorytm zapamiętuje wszystkie dane treningowe i nie buduje żadnego modelu wstępnego.\n",
    "\n",
    "2. **Dla nowej próbki (punktu testowego)**:\n",
    "   - **Krok 1**: Oblicz odległości między nową próbką a wszystkimi punktami w zbiorze treningowym.\n",
    "     - Najczęściej używane metryki odległości to:\n",
    "       - **Odległość euklidesowa**:\n",
    "         $$\n",
    "         d(x, x') = \\sqrt{\\sum_{i=1}^n (x_i - x'_i)^2}\n",
    "         $$\n",
    "       - **Odległość Manhattan**:\n",
    "         $$\n",
    "         d(x, x') = \\sum_{i=1}^n |x_i - x'_i|\n",
    "         $$\n",
    "   - **Krok 2**: Wybierz \\( k \\) najbliższych sąsiadów (najmniejszych odległości) do punktu testowego.\n",
    "\n",
    "3. **Obliczanie wartości przewidywanej**:\n",
    "   - Wartość przewidywana dla punktu testowego to **średnia wartość wyjściowa \\( y \\)** najbliższych sąsiadów:\n",
    "     $$\n",
    "     \\hat{y} = \\frac{1}{k} \\sum_{i=1}^k y_i\n",
    "     $$\n",
    "   - Możliwe jest również stosowanie wag, gdzie bliżsi sąsiedzi mają większy wpływ na wynik:\n",
    "     $$\n",
    "     \\hat{y} = \\frac{\\sum_{i=1}^k w_i \\cdot y_i}{\\sum_{i=1}^k w_i}\n",
    "     $$\n",
    "     gdzie $ w_i $ to odwrotność odległości do sąsiada $ w_i = \\frac{1}{d(x, x'_i) + \\epsilon} $, $ \\epsilon $ to mała wartość zapobiegająca dzieleniu przez zero.\n",
    "\n",
    "4. **Powtórzenie dla wszystkich punktów testowych**:\n",
    "   - Algorytm wykonuje powyższe kroki dla każdej próbki w zbiorze testowym.\n",
    "\n",
    "---\n",
    "\n",
    "## **Hiperparametry k-NN**\n",
    "\n",
    "1. **Liczba sąsiadów $k$**:\n",
    "   - Małe $ k $:\n",
    "     - Model bardziej wrażliwy na szum w danych (przejawia tendencję do overfittingu).\n",
    "   - Duże $ k $:\n",
    "     - Model uśrednia wartości w szerszym zakresie, co może prowadzić do underfittingu.\n",
    "\n",
    "2. **Metryka odległości**:\n",
    "   - Najczęściej używana jest odległość euklidesowa, ale można stosować inne miary, takie jak Manhattan, Minkowski czy metryki specjalistyczne.\n",
    "\n",
    "3. **Wagi sąsiadów**:\n",
    "   - Domyślnie wszyscy sąsiedzi mają równą wagę.\n",
    "   - Można stosować wagi zależne od odległości, aby bliżsi sąsiedzi mieli większy wpływ.\n",
    "\n",
    "---\n",
    "\n",
    "## **Zalety algorytmu k-NN w regresji**\n",
    "\n",
    "1. **Prostota**:\n",
    "   - Algorytm jest łatwy do zrozumienia i implementacji.\n",
    "\n",
    "2. **Elastyczność**:\n",
    "   - Działa dobrze z danymi o skomplikowanych i nieliniowych zależnościach.\n",
    "\n",
    "3. **Brak założeń**:\n",
    "   - Nie wymaga wstępnych założeń o rozkładzie danych.\n",
    "\n",
    "---\n",
    "\n",
    "## **Wady algorytmu k-NN w regresji**\n",
    "\n",
    "1. **Koszt obliczeniowy**:\n",
    "   - Algorytm wymaga obliczania odległości dla wszystkich punktów w zbiorze treningowym dla każdej próbki testowej, co może być kosztowne dla dużych zbiorów danych.\n",
    "\n",
    "2. **Wrażliwość na dane**:\n",
    "   - Wrażliwość na szum i wartości odstające, szczególnie dla małych wartości $ k $.\n",
    "\n",
    "3. **Problemy ze skalą danych**:\n",
    "   - Algorytm jest wrażliwy na różnice w skali cech, dlatego standaryzacja lub normalizacja danych jest zwykle konieczna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f93381fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1f321923ed4ef9b5b2aebbaee73fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='k (Neighbors)', max=20, min=1), RadioButtons(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_knn(n_neighbors=3, weights='uniform')>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=20, random_state=42)\n",
    "X_train, X_test = X[:160], X[160:]\n",
    "y_train, y_test = y[:160], y[160:]\n",
    "\n",
    "# Interactive function for k-NN\n",
    "def interactive_knn(n_neighbors=3, weights='uniform'):\n",
    "    # Train k-NN regressor\n",
    "    knn = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Plot training data, test data, and predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Scatter plot of training and test data\n",
    "    plt.scatter(X_train, y_train, label=\"Training Data\", color='blue', alpha=0.7)\n",
    "    plt.scatter(X_test, y_test, label=\"Test Data\", color='orange', alpha=0.7)\n",
    "    \n",
    "    # Line plot of predictions\n",
    "    X_range = np.linspace(X.min(), X.max(), 500).reshape(-1, 1)\n",
    "    y_pred_range = knn.predict(X_range)\n",
    "    plt.plot(X_range, y_pred_range, color='red', label=\"k-NN Predictions\", linewidth=2)\n",
    "    \n",
    "    # Titles and labels\n",
    "    plt.title(f\"k-NN Regression (k={n_neighbors}, weights='{weights}')\\nMSE: {mse:.2f}\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Target\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widgets for k and weights\n",
    "interact(\n",
    "    interactive_knn,\n",
    "    n_neighbors=widgets.IntSlider(min=1, max=20, step=1, value=3, description='k (Neighbors)'),\n",
    "    weights=widgets.RadioButtons(\n",
    "        options=['uniform', 'distance'],\n",
    "        value='uniform',\n",
    "        description='Weights:'\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cb426",
   "metadata": {},
   "source": [
    "## Zadanie 3.\n",
    "\n",
    "Wykorzystując dane z poprzednich ćwiczeń (i podział na zbiory oraz sprawdzian kryżowy), wykorzystaj algorytm k-NN do predykcji. Wesprzyj się dokumentacją: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "    3a. Sprawdź wynik k-NN dla wybranego przez Ciebie zestawu parametrów.\n",
    "    3b. Dla wybranego zestawu cech, sprawdź różne kombinacje hiperparametrów i wykonaj krzywe złożoności dla parametry n_neighbors oraz p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c5f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0682eebc",
   "metadata": {},
   "source": [
    "# Drzewa decyzyjne\n",
    "\n",
    "Drzewa decyzyjne to popularny algorytm uczenia maszynowego, który można wykorzystać zarówno do zadań klasyfikacji, jak i regresji. W przypadku regresji drzewa decyzyjne służą do przewidywania wartości ciągłych, co czyni je potężnym narzędziem w modelowaniu danych z nieliniowymi i złożonymi zależnościami.\n",
    "\n",
    "\n",
    "## Drzewo decyzyjne\n",
    "\n",
    "Drzewo decyzyjne to struktura przypominająca diagram, w której dane są dzielone na podstawie warunków logicznych, tworząc węzły i gałęzie.\n",
    "\n",
    "    Korzeń drzewa: Punkt początkowy, zawierający całe dane wejściowe.\n",
    "    \n",
    "    Węzły decyzyjne: Punkty, w których dane są dzielone na podstawie określonego warunku (np. cechy, która najlepiej wyjaśnia dane w danym momencie).\n",
    "    \n",
    "    Liście: Końcowe węzły, które reprezentują przewidywaną wartość w przypadku regresji.\n",
    "    \n",
    "## Regresja z wykorzystaniem drzew decyzyjnych\n",
    "\n",
    "### Podział danych - proces uczenia\n",
    "\n",
    "Drzewo iteracyjnie dzieli dane wejściowe na podzbiory na podstawie wybranej cechy i warunku, który minimalizuje błąd predykcji.\n",
    "\n",
    "    Na przykład powierzchnia mieszkania > 50m2 -> tak lub nie, dzieli dane na dwa podzbiory w których predykcja ma konkretną wartość.\n",
    "\n",
    "Używa się miar takich jak średni błąd kwadratowy (MSE) lub średni absolutny błąd (MAE), aby określić najlepszy podział.\n",
    "\n",
    "    Kryterium podziału to róznica między rzeczywistą wartością a średnią wartością w podgrupie, która powstała.\n",
    "\n",
    "Algorytm kontynuuje podziały, aż osiągnie określone kryterium stopu, np. minimalną liczbę próbek w liściu, maksymalną głębokość drzewa lub brak dalszej poprawy błędu.\n",
    "\n",
    "### Predykcja\n",
    "Wartość przewidywana w liściu drzewa decyzyjnego to średnia wartość wyników w danym podzbiorze danych.\n",
    "\n",
    "### Zalety\n",
    "    Drzewa są bardzo proste w użyciu i intuicyjne\n",
    "    Są zdolne do obsługi wysoce nieliniowych związków\n",
    "    Nie ma potrzeby skalowania danych\n",
    "    \n",
    "### Wady\n",
    "\n",
    "    Drzewa są podatne na overfitting\n",
    "    Ich przewidywania są średnimi, a zatem są skokowe\n",
    "    \n",
    "### Kiedy stosować\n",
    "\n",
    "    Gdy dane są wysoce nielinowe\n",
    "    Gdy interpretowalność modelu jest kluczowa\n",
    "    W danych występują złożone interakcje między cechami, a które trudno ująć np. regresją wielomianową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3bba0e99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33934ad8e2a744b6a453a3c5359e35d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='Max Depth', max=10, min=1), IntSlider(value=2, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_tree_with_plot(max_depth=3, min_samples_split=2, min_samples_leaf=1)>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import additional library for tree visualization\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Adjust interactive function to include tree visualization\n",
    "def interactive_tree_with_plot(max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
    "    # Train the Decision Tree Regressor with selected hyperparameters\n",
    "    regressor = DecisionTreeRegressor(\n",
    "        max_depth=max_depth, \n",
    "        min_samples_split=min_samples_split, \n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the training and test set\n",
    "    y_pred_train = regressor.predict(X_train)\n",
    "    y_pred_test = regressor.predict(X_test)\n",
    "    \n",
    "    # Calculate Mean Squared Error\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    \n",
    "    # Create subplots for predictions and tree structure\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Plot the data and the regression tree predictions\n",
    "    X_range = np.linspace(X.min(), X.max(), 500).reshape(-1, 1)\n",
    "    y_pred_range = regressor.predict(X_range)\n",
    "    \n",
    "    axs[0].scatter(X_train, y_train, color='blue', label='Training Data', alpha=0.6)\n",
    "    axs[0].scatter(X_test, y_test, color='orange', label='Test Data', alpha=0.6)\n",
    "    axs[0].plot(X_range, y_pred_range, color='red', label='Tree Prediction', linewidth=2)\n",
    "    axs[0].set_title(f\"Decision Tree Regressor\\nTrain MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}\")\n",
    "    axs[0].set_xlabel(\"Feature\")\n",
    "    axs[0].set_ylabel(\"Target\")\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Visualize the decision tree structure\n",
    "    plot_tree(regressor, filled=True, feature_names=[\"Feature\"], ax=axs[1])\n",
    "    axs[1].set_title(\"Decision Tree Structure\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widgets for hyperparameters\n",
    "interact(\n",
    "    interactive_tree_with_plot,\n",
    "    max_depth=widgets.IntSlider(min=1, max=10, step=1, value=3, description='Max Depth'),\n",
    "    min_samples_split=widgets.IntSlider(min=2, max=20, step=1, value=2, description='Min Samples Split'),\n",
    "    min_samples_leaf=widgets.IntSlider(min=1, max=10, step=1, value=1, description='Min Samples Leaf')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1f62179f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d1bf8d9f114e81b399bbc03cea194d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='Max Depth', max=10, min=1), IntSlider(value=2, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_tree_with_plot_nonlinear(max_depth=3, min_samples_split=2, min_samples_leaf=1)>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a more non-linear dataset\n",
    "np.random.seed(42)\n",
    "X_nl = np.sort(np.random.rand(200, 1) * 10, axis=0)  # Random values between 0 and 10\n",
    "y_nl = np.sin(X_nl).ravel() + np.random.normal(0, 0.2, X_nl.shape[0])  # Sinusoidal with noise\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train_nl, X_test_nl, y_train_nl, y_test_nl = train_test_split(X_nl, y_nl, test_size=0.2, random_state=42)\n",
    "\n",
    "# Interactive function for non-linear example\n",
    "def interactive_tree_with_plot_nonlinear(max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
    "    # Train the Decision Tree Regressor with selected hyperparameters\n",
    "    regressor = DecisionTreeRegressor(\n",
    "        max_depth=max_depth, \n",
    "        min_samples_split=min_samples_split, \n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    regressor.fit(X_train_nl, y_train_nl)\n",
    "    \n",
    "    # Predict on the training and test set\n",
    "    y_pred_train_nl = regressor.predict(X_train_nl)\n",
    "    y_pred_test_nl = regressor.predict(X_test_nl)\n",
    "    \n",
    "    # Calculate Mean Squared Error\n",
    "    train_mse = mean_squared_error(y_train_nl, y_pred_train_nl)\n",
    "    test_mse = mean_squared_error(y_test_nl, y_pred_test_nl)\n",
    "    \n",
    "    # Create subplots for predictions and tree structure\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(18, 16))\n",
    "    \n",
    "    # Plot the data and the regression tree predictions\n",
    "    X_range = np.linspace(X_nl.min(), X_nl.max(), 500).reshape(-1, 1)\n",
    "    y_pred_range = regressor.predict(X_range)\n",
    "    \n",
    "    axs[0].scatter(X_train_nl, y_train_nl, color='blue', label='Training Data', alpha=0.6)\n",
    "    axs[0].scatter(X_test_nl, y_test_nl, color='orange', label='Test Data', alpha=0.6)\n",
    "    axs[0].plot(X_range, y_pred_range, color='red', label='Tree Prediction', linewidth=2)\n",
    "    axs[0].set_title(f\"Decision Tree Regressor (Non-Linear Example)\\nTrain MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}\")\n",
    "    axs[0].set_xlabel(\"Feature\")\n",
    "    axs[0].set_ylabel(\"Target\")\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Visualize the decision tree structure\n",
    "    plot_tree(regressor, filled=True, feature_names=[\"Feature\"], ax=axs[1])\n",
    "    axs[1].set_title(\"Decision Tree Structure\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widgets for hyperparameters\n",
    "interact(\n",
    "    interactive_tree_with_plot_nonlinear,\n",
    "    max_depth=widgets.IntSlider(min=1, max=10, step=1, value=3, description='Max Depth'),\n",
    "    min_samples_split=widgets.IntSlider(min=2, max=20, step=1, value=2, description='Min Samples Split'),\n",
    "    min_samples_leaf=widgets.IntSlider(min=1, max=10, step=1, value=1, description='Min Samples Leaf')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2927970",
   "metadata": {},
   "source": [
    "## Zadanie 4.\n",
    "\n",
    "Wykorzystując dane z poprzednich ćwiczeń (i podział na zbiory oraz sprawdzian kryżowy), wykorzystaj algorytm k-NN do predykcji. Wesprzyj się dokumentacją: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "\n",
    "    4a. Wytrenuj model dla danych dobierając parametry na podstawie krzywych złożoności.\n",
    "    4b. Dla wybranego modelu zwizualizuj drzewo i dokonaj jego interpretacji.\n",
    "    4c. Przeanalizuj predykcje, w których przewidywana wartość znacząco odstaje od predykcji drzewa. Spróbuj zinterpretować przyczynę tego zdarzenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc078c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
